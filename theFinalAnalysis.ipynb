{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujHhwdAGvRRs",
        "outputId": "e9857a00-013e-4ab8-9040-5b7478ab8bfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libgl1-mesa-glx is already the newest version (23.0.4-0ubuntu1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U pymupdf llama-index-core llama-index-llms-llama-cpp llama-index-embeddings-huggingface llama-index-retrievers-bm25 langchain langchain-openai sentence-transformers google-generativeai\n",
        "!apt-get install -y libgl1-mesa-glx\n",
        "!pip install llama-index-llms-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76PZXHvavCTj",
        "outputId": "eaf6ff34-df82-46cc-eb9c-a88c0ed9116b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import fitz\n",
        "import torch\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from getpass import getpass\n",
        "from collections import Counter\n",
        "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response_synthesizers import CompactAndRefine\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
        "from llama_index.core.postprocessor import LLMRerank\n",
        "from llama_index.core.schema import QueryBundle, NodeWithScore\n",
        "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from llama_index.core import VectorStoreIndex, Document\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "\n",
        "# Verify CUDA availability\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "Xe88wkeksb3h",
        "outputId": "043a9640-85ef-4058-905d-191ec6fc79f5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b16e72b0-b9bd-43c6-baf6-3d7dc930058f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b16e72b0-b9bd-43c6-baf6-3d7dc930058f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving sample_bank_statement.pdf to sample_bank_statement (1).pdf\n",
            "Saving payslip_sample_image.pdf to payslip_sample_image (1).pdf\n",
            "Saving appraisal_report.pdf to appraisal_report (1).pdf\n",
            "Saving LenderFeesWorksheetNew.pdf to LenderFeesWorksheetNew (2).pdf\n",
            "Extracted 287 words from sample_bank_statement (1).pdf\n",
            "Extracted 82 words from payslip_sample_image (1).pdf\n",
            "Extracted 6470 words from appraisal_report (1).pdf\n",
            "Extracted 404 words from LenderFeesWorksheetNew (2).pdf\n"
          ]
        }
      ],
      "source": [
        "# 2. Document Upload & Setup\n",
        "\n",
        "# Upload PDF documents\n",
        "uploaded = files.upload()\n",
        "doc_paths = {\n",
        "    \"Unknown 1\": list(uploaded.keys())[0],\n",
        "    \"Unknown 2\": list(uploaded.keys())[1],\n",
        "    \"Unknown 3\": list(uploaded.keys())[2],\n",
        "    \"Unknown 4\": list(uploaded.keys())[3],\n",
        "}\n",
        "\n",
        "# Extract text from uploaded PDFs\n",
        "doc_texts = {}\n",
        "for i, (doc_type, filename) in enumerate(doc_paths.items()):\n",
        "    with fitz.open(filename) as doc:\n",
        "        text = \"\\n\".join([page.get_text() for page in doc])\n",
        "        doc_texts[f\"Doc-{i+1}\"] = text\n",
        "    print(f\"Extracted {len(text.split())} words from {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA3eiLJtseXP",
        "outputId": "2e7de501-7082-478b-a67a-f4595e43f364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API key: ··········\n",
            "Enter Google API key: ··········\n"
          ]
        }
      ],
      "source": [
        "# 3. Document Classification Components\n",
        "\n",
        "# %%\n",
        "# Set API keys\n",
        "os.environ['OPENAI_API_KEY'] = getpass(\"Enter OpenAI API key: \")\n",
        "os.environ['GOOGLE_API_KEY'] = getpass(\"Enter Google API key: \")\n",
        "\n",
        "def prepare_document_for_classification(text):\n",
        "    # Normalize text\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Get document length\n",
        "    doc_length = len(text)\n",
        "\n",
        "    # Extract first, middle, and last portions\n",
        "    first_part = text[:min(500, doc_length)]\n",
        "    middle_start = max(0, doc_length // 2 - 250)\n",
        "    middle_part = text[middle_start:middle_start + min(500, doc_length - middle_start)]\n",
        "    last_start = max(0, doc_length - 500)\n",
        "    last_part = text[last_start:]\n",
        "\n",
        "    # Extract structural elements\n",
        "    headers = re.findall(r'(?:^|\\n)([A-Z\\s]{5,50})(?:\\n|$)', text)\n",
        "    headers = headers[:10]  # Limit to first 10 headers\n",
        "\n",
        "    tables = re.findall(r'\\n\\|.*\\|.*\\|\\n', text)\n",
        "    table_count = len(tables)\n",
        "\n",
        "    dates = re.findall(r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b', text)\n",
        "    amounts = re.findall(r'\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d{2})?', text)\n",
        "    names = re.findall(r'\\b([A-Z][a-z]+ [A-Z][a-z]+)\\b', text)\n",
        "\n",
        "    mortgage_keywords = ['mortgage', 'loan', 'interest', 'principal', 'amortization', 'lender', 'borrower', 'payment', 'contract', 'agreement', 'payslip', 'salary', 'income']\n",
        "    keywords_found = [word for word in mortgage_keywords if word.lower() in text.lower()]\n",
        "\n",
        "    word_freq = Counter(text.lower().split())\n",
        "    top_words = dict(word_freq.most_common(10))\n",
        "\n",
        "    return {\n",
        "        \"first_part\": first_part,\n",
        "        \"middle_part\": middle_part,\n",
        "        \"last_part\": last_part,\n",
        "        \"total_length\": doc_length,\n",
        "        \"potential_headers\": \"\\n\".join(headers),\n",
        "        \"table_count\": table_count,\n",
        "        \"dates\": dates,\n",
        "        \"amounts\": amounts,\n",
        "        \"names\": names,\n",
        "        \"keywords\": keywords_found,\n",
        "        \"top_words\": top_words\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1kGm5UJuKhH"
      },
      "outputs": [],
      "source": [
        "def classify_document(text, llm):\n",
        "    doc_info = prepare_document_for_classification(text)\n",
        "\n",
        "    categories = [\n",
        "        \"Bank Statement\", \"Pay Slip\", \"Appraisal Report\", \"Loan Agreement\",\n",
        "        \"Mortgage Contract\", \"Credit Report\", \"Employment Verification\",\n",
        "        \"Tax Return\", \"Insurance Policy\", \"Title Report\", \"Unknown\"\n",
        "    ]\n",
        "\n",
        "    prompt = f\"\"\"You are a document classification expert. Classify this document into one of these categories:\n",
        "    {', '.join(categories[:-1])} or {categories[-1]}\n",
        "\n",
        "    Here's information extracted from the document:\n",
        "\n",
        "    DOCUMENT START EXCERPT:\n",
        "    {doc_info['first_part']}\n",
        "    DOCUMENT START EXCERPT END\n",
        "\n",
        "    DOCUMENT MIDDLE EXCERPT:\n",
        "    {doc_info['middle_part']}\n",
        "    DOCUMENT MIDDLE EXCERPT END\n",
        "\n",
        "    DOCUMENT END EXCERPT:\n",
        "    {doc_info['last_part']}\n",
        "    DOCUMENT END EXCERPT END\n",
        "\n",
        "    Total document length: {doc_info['total_length']} characters\n",
        "\n",
        "    Additional Information:\n",
        "    - Potential Headers: {doc_info['potential_headers']}\n",
        "    - Number of Tables: {doc_info['table_count']}\n",
        "    - Dates Found: {', '.join(doc_info['dates']) if doc_info['dates'] else 'None'}\n",
        "    - Amounts Found: {', '.join(doc_info['amounts']) if doc_info['amounts'] else 'None'}\n",
        "    - Names Found: {', '.join(doc_info['names']) if doc_info['names'] else 'None'}\n",
        "    - Keywords: {', '.join(doc_info['keywords']) if doc_info['keywords'] else 'None'}\n",
        "    - Top 10 Words: {', '.join([f'{k}: {v}' for k, v in doc_info['top_words'].items()])}\n",
        "\n",
        "    IMPORTANT INSTRUCTION: Your response must be EXACTLY ONE of these options:\n",
        "    {', '.join(categories)}\n",
        "\n",
        "    Do not include any explanation, reasoning, or additional text. Respond with ONLY the category name.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.complete(prompt)\n",
        "    raw_response = response.text.strip()\n",
        "\n",
        "    if raw_response in categories:\n",
        "        return raw_response\n",
        "\n",
        "    for category in categories:\n",
        "        if category.lower() in raw_response.lower():\n",
        "            return category\n",
        "\n",
        "    words = raw_response.lower().split()\n",
        "    keyword_map = {\n",
        "        \"Bank Statement\": ['bank', 'statement', 'account', 'transaction'],\n",
        "        \"Pay Slip\": ['pay', 'slip', 'salary', 'income', 'wage'],\n",
        "        \"Appraisal Report\": ['appraisal', 'property', 'valuation', 'assessment'],\n",
        "        \"Loan Agreement\": ['loan', 'agreement', 'borrower', 'lender'],\n",
        "        \"Mortgage Contract\": ['mortgage', 'contract', 'deed', 'lien'],\n",
        "        \"Credit Report\": ['credit', 'report', 'score', 'history'],\n",
        "        \"Employment Verification\": ['employment', 'verification', 'job', 'employer'],\n",
        "        \"Tax Return\": ['tax', 'return', 'irs', 'income'],\n",
        "        \"Insurance Policy\": ['insurance', 'policy', 'coverage', 'premium'],\n",
        "        \"Title Report\": ['title', 'report', 'ownership', 'lien']\n",
        "    }\n",
        "\n",
        "    for category, keywords in keyword_map.items():\n",
        "        if any(kw in words for kw in keywords) or any(kw in doc_info['keywords'] for kw in keywords):\n",
        "            return category\n",
        "\n",
        "    for keyword in doc_info['keywords']:\n",
        "        for category, keywords in keyword_map.items():\n",
        "            if keyword in keywords:\n",
        "                return category\n",
        "\n",
        "    return \"Unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FlPTaqIsh27"
      },
      "outputs": [],
      "source": [
        "# 4. Query Processing Components\n",
        "\n",
        "# Query rewriter setup\n",
        "re_write_llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name=\"gpt-4o\",\n",
        "    max_tokens=4000,\n",
        "    api_key=os.environ['OPENAI_API_KEY']\n",
        ")\n",
        "\n",
        "query_rewrite_template = \"\"\"You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system.\n",
        "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
        "\n",
        "Original query: {original_query}\n",
        "\n",
        "Rewritten query:\"\"\"\n",
        "\n",
        "query_rewrite_prompt = PromptTemplate(\n",
        "    input_variables=[\"original_query\"],\n",
        "    template=query_rewrite_template\n",
        ")\n",
        "\n",
        "query_rewriter = query_rewrite_prompt | re_write_llm\n",
        "\n",
        "def rewrite_query(original_query):\n",
        "    \"\"\"\n",
        "    Rewrite the original query to improve retrieval.\n",
        "\n",
        "    Args:\n",
        "    original_query (str): The original user query\n",
        "\n",
        "    Returns:\n",
        "    str: The rewritten query\n",
        "    \"\"\"\n",
        "    response = query_rewriter.invoke(original_query)\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUYRA0G1usaD"
      },
      "outputs": [],
      "source": [
        "def route_query(query, llm, classified_docs):\n",
        "    # Expanded list of categories for mortgage-related documents\n",
        "    categories = [\n",
        "        \"Bank Statement\", \"Pay Slip\", \"Appraisal Report\", \"Loan Agreement\",\n",
        "        \"Mortgage Contract\", \"Credit Report\", \"Employment Verification\",\n",
        "        \"Tax Return\", \"Insurance Policy\", \"Title Report\", \"Unknown\"\n",
        "    ]\n",
        "\n",
        "    # Check which document type the query is related to\n",
        "    prompt = f\"\"\"\n",
        "    Classify the following question into one of these categories:\n",
        "    {', '.join(categories[:-1])} or {categories[-1]}\n",
        "\n",
        "    IMPORTANT INSTRUCTION: Your response must be EXACTLY ONE of these options:\n",
        "    {', '.join(categories)}\n",
        "\n",
        "    Do not include any explanation, reasoning, or additional text. Respond with ONLY the category name.\n",
        "\n",
        "    Query: {query}\n",
        "    \"\"\"\n",
        "\n",
        "    doc_type = llm.complete(prompt).text.strip()\n",
        "\n",
        "    raw_response = doc_type\n",
        "\n",
        "    # Post-process to extract just the category name\n",
        "    if raw_response in categories:\n",
        "        doc_type = raw_response\n",
        "\n",
        "    # If not, look for the category within the response\n",
        "    for category in categories:\n",
        "        if category.lower() in raw_response.lower():\n",
        "            doc_type = category\n",
        "\n",
        "    # If still no match, return the closest match based on keywords\n",
        "    words = raw_response.lower().split()\n",
        "    keyword_map = {\n",
        "        \"Bank Statement\": ['bank', 'statement', 'account', 'transaction'],\n",
        "        \"Pay Slip\": ['pay', 'slip', 'salary', 'income', 'wage'],\n",
        "        \"Appraisal Report\": ['appraisal', 'property', 'valuation', 'assessment'],\n",
        "        \"Loan Agreement\": ['loan', 'agreement', 'borrower', 'lender'],\n",
        "        \"Contract\": ['agreement', 'contract', 'mortgage', 'deed', 'lien'],\n",
        "        \"Credit Report\": ['credit', 'report', 'score', 'history'],\n",
        "        \"Employment Verification\": ['employment', 'verification', 'job', 'employer'],\n",
        "        \"Tax Return\": ['tax', 'return', 'irs', 'income'],\n",
        "        \"Insurance Policy\": ['insurance', 'policy', 'coverage', 'premium'],\n",
        "        \"Title Report\": ['title', 'report', 'ownership', 'lien']\n",
        "    }\n",
        "\n",
        "    for category, keywords in keyword_map.items():\n",
        "        if any(kw in words for kw in keywords):\n",
        "            doc_type = category\n",
        "            break\n",
        "    else:\n",
        "        doc_type = \"Unknown\"\n",
        "\n",
        "     # Filter documents based on the determined document type\n",
        "    relevant_docs = [\n",
        "        doc_data[\"document\"] for doc_data in classified_docs.values()\n",
        "        if doc_data[\"doc_type\"] == doc_type\n",
        "    ]\n",
        "\n",
        "    # Debug print to check if any documents were found\n",
        "    print(f\"Number of relevant documents found for query '{query}': {len(relevant_docs)}\")\n",
        "\n",
        "    # Fallback mechanism: If no relevant documents are found, use all documents\n",
        "    if not relevant_docs:\n",
        "        print(\"No relevant documents found. Using all documents as fallback.\")\n",
        "        relevant_docs = [doc_data[\"document\"] for doc_data in classified_docs.values()]\n",
        "\n",
        "    return relevant_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCtGOhqpsj5O"
      },
      "outputs": [],
      "source": [
        "# 5. Retrieval & Ranking Setup\n",
        "\n",
        "# Download model for LlamaCPP\n",
        "!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf -O mistral-7b-instruct.gguf\n",
        "\n",
        "# Initialize models\n",
        "llm = LlamaCPP(\n",
        "    model_path=\"mistral-7b-instruct.gguf\",\n",
        "    temperature=0.0,\n",
        "    max_new_tokens=30,\n",
        "    context_window=4096,\n",
        "    model_kwargs={\"n_gpu_layers\": 1, \"verbose\": False}\n",
        ")\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "def semantic_chunk(docs, embed_model):\n",
        "    \"\"\"Modified to handle LlamaIndex Documents directly\"\"\"\n",
        "    # Create a sentence splitter for initial text splitting\n",
        "    sentence_splitter = SentenceSplitter(chunk_size=512)\n",
        "\n",
        "    # Create a semantic splitter for more meaningful chunks\n",
        "    semantic_splitter = SemanticSplitterNodeParser(\n",
        "        buffer_size=1,\n",
        "        breakpoint_percentile_threshold=95,\n",
        "        embed_model=embed_model\n",
        "    )\n",
        "\n",
        "    nodes = []\n",
        "    for doc in docs:\n",
        "        # Split document into sentences first\n",
        "        split_sentences = sentence_splitter.get_nodes_from_documents([doc])\n",
        "\n",
        "        # Then apply semantic splitting to the sentences\n",
        "        semantic_nodes = semantic_splitter.get_nodes_from_documents(split_sentences)\n",
        "        nodes.extend(semantic_nodes)\n",
        "\n",
        "    print(f\"Number of {docs[0].metadata['doc_type']} nodes after splitting: {len(nodes)}\")\n",
        "    top_k_bm25 = min(3, len(nodes))\n",
        "    return nodes, top_k_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g1jelOUu5Ol"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "class HybridRetriever(BaseRetriever):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vector_retriever,\n",
        "        bm25_retriever,\n",
        "        vector_weight: float = 0.5,\n",
        "        top_k_per_retriever: int = 10,\n",
        "        top_n: int = 10,\n",
        "        dedup_threshold: float = 0.9,\n",
        "    ):\n",
        "        self.vector_retriever = vector_retriever\n",
        "        self.bm25_retriever = bm25_retriever\n",
        "        self.vector_weight = vector_weight\n",
        "        self.top_k_per_retriever = top_k_per_retriever\n",
        "        self.top_n = top_n\n",
        "        self.dedup_threshold = dedup_threshold\n",
        "        super().__init__()\n",
        "\n",
        "    def _normalize_scores(self, nodes: List[NodeWithScore]) -> List[NodeWithScore]:\n",
        "        if not nodes:\n",
        "            return []\n",
        "        scores = [node.score for node in nodes]\n",
        "        min_score, max_score = min(scores), max(scores)\n",
        "        for node in nodes:\n",
        "            if max_score - min_score == 0:\n",
        "                node.score = 0.0\n",
        "            else:\n",
        "                node.score = (node.score - min_score) / (max_score - min_score)\n",
        "        return nodes\n",
        "\n",
        "    def _deduplicate_nodes(self, nodes: List[NodeWithScore]) -> List[NodeWithScore]:\n",
        "        deduped = []\n",
        "        seen_ids = set()\n",
        "        for node in nodes:\n",
        "            if node.node.node_id not in seen_ids:\n",
        "                seen_ids.add(node.node.node_id)\n",
        "                deduped.append(node)\n",
        "        return deduped\n",
        "\n",
        "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
        "        # Parallel retrieval (example using threading)\n",
        "        import threading\n",
        "        vector_results = []\n",
        "        bm25_results = []\n",
        "\n",
        "        def fetch_vector():\n",
        "            nonlocal vector_results\n",
        "            vector_results = self.vector_retriever.retrieve(query_bundle)[:self.top_k_per_retriever]\n",
        "\n",
        "        def fetch_bm25():\n",
        "            nonlocal bm25_results\n",
        "            bm25_results = self.bm25_retriever.retrieve(query_bundle)[:self.top_k_per_retriever]\n",
        "\n",
        "        t1 = threading.Thread(target=fetch_vector)\n",
        "        t2 = threading.Thread(target=fetch_bm25)\n",
        "        t1.start()\n",
        "        t2.start()\n",
        "        t1.join()\n",
        "        t2.join()\n",
        "\n",
        "        # Normalize and fuse scores\n",
        "        vector_nodes = self._normalize_scores(vector_results)\n",
        "        bm25_nodes = self._normalize_scores(bm25_results)\n",
        "\n",
        "        combined_nodes = []\n",
        "        for node in vector_nodes:\n",
        "            combined_score = self.vector_weight * node.score\n",
        "            combined_nodes.append((node, combined_score))\n",
        "        for node in bm25_nodes:\n",
        "            combined_score = (1 - self.vector_weight) * node.score\n",
        "            combined_nodes.append((node, combined_score))\n",
        "\n",
        "        # Sort by combined score and deduplicate\n",
        "        combined_nodes.sort(key=lambda x: x[1], reverse=True)\n",
        "        deduped_nodes = self._deduplicate_nodes([node for node, _ in combined_nodes])\n",
        "\n",
        "        # Apply final top_n limit\n",
        "        return deduped_nodes[:self.top_n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF4vqHjNsmS3",
        "outputId": "1af0fd00-4bb8-458d-b669-6ab135f6e9b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CLASSIFYING DOCUMENTS...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-ef740b95f0d5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclassified_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdoc_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     classified_docs[doc_id] = {\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-f1594d341377>\u001b[0m in \u001b[0;36mclassify_document\u001b[0;34m(text, llm)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \"\"\"\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mraw_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/instrumentation/dispatcher.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0;31m# If the result is a Future, wrap it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/llms/callbacks.py\u001b[0m in \u001b[0;36mwrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                 )\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m                     \u001b[0mf_return_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                     callback_manager.on_event_end(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/llms/llama_cpp/base.py\u001b[0m in \u001b[0;36mcomplete\u001b[0;34m(self, prompt, formatted, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletion_to_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCompletionResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0mResponse\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m         \"\"\"\n\u001b[0;32m-> 1902\u001b[0;31m         return self.create_completion(\n\u001b[0m\u001b[1;32m   1903\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m             \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mcreate_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1833\u001b[0m             \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCreateCompletionStreamResponse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion_or_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1835\u001b[0;31m         \u001b[0mcompletion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion_or_chunks\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1836\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m_create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0mfinish_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"length\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0mmultibyte_fix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         for token in self.generate(\n\u001b[0m\u001b[1;32m   1321\u001b[0m             \u001b[0mprompt_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0;31m# Eval and sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msample_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                 token = self.sample(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             )\n\u001b[0;32m--> 646\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m             \u001b[0;31m# Save tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_past\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mn_past\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_tokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp/_internals.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLlamaBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         return_code = llama_cpp.llama_decode(\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 6. Document Classification Execution\n",
        "\n",
        "# Classify documents\n",
        "print(\"CLASSIFYING DOCUMENTS...\")\n",
        "classified_docs = {}\n",
        "for doc_id, text in doc_texts.items():\n",
        "    doc_type = classify_document(text, llm)\n",
        "    classified_docs[doc_id] = {\n",
        "        \"text\": text,\n",
        "        \"doc_type\": doc_type,\n",
        "        \"document\": Document(text=text, metadata={\"doc_type\": doc_type})\n",
        "    }\n",
        "    print(f\"Classified {doc_id} as: {doc_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPEaU_MusoRr"
      },
      "outputs": [],
      "source": [
        "# 7. Query Execution\n",
        "\n",
        "# Process query\n",
        "print(\"\\nPROCESSING QUERY...\")\n",
        "original_query = input(\"Enter your query: \")\n",
        "rewritten_query = rewrite_query(original_query)\n",
        "print(f\"Original: {original_query}\\nRewritten: {rewritten_query}\")\n",
        "\n",
        "# Route query\n",
        "print(\"\\nQUERY ROUTING...\")\n",
        "query_route_results = route_query(rewritten_query, llm, classified_docs)\n",
        "\n",
        "# Prepare retrieval system\n",
        "nodes, top_k_bm25 = semantic_chunk(query_route_results, embed_model)\n",
        "docstore = SimpleDocumentStore()\n",
        "docstore.add_documents(nodes)\n",
        "\n",
        "vector_index = VectorStoreIndex.from_documents(query_route_results, embed_model=embed_model)\n",
        "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=5)\n",
        "bm25_retriever = BM25Retriever.from_defaults(docstore=docstore, similarity_top_k=3)\n",
        "\n",
        "hybrid_retriever = HybridRetriever(\n",
        "    vector_retriever=vector_retriever,\n",
        "    bm25_retriever=bm25_retriever\n",
        ")\n",
        "\n",
        "# Final query execution\n",
        "llm = GoogleGenAI(model=\"gemini-1.5-flash\")\n",
        "query_engine = RetrieverQueryEngine.from_args(hybrid_retriever, llm=llm)\n",
        "response = query_engine.query(rewritten_query)\n",
        "\n",
        "print(\"\\nFINAL RESPONSE:\")\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}