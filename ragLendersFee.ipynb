{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgN25wzNxu3F"
      },
      "source": [
        "Install necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ftOgVVeJejen"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index llama-index-llms-openai pymupdf llama-index-embeddings-huggingface langchain langchain_openai\n",
        "!pip install llama-index-retrievers-bm25\n",
        "!pip install llama-index-readers-file pymupdf\n",
        "!pip install --upgrade sympy\n",
        "!pip install llama_index.llms.gemini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr9dyrqNxxN4"
      },
      "source": [
        "Load document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1jIIp_Hg1f_"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Load PDF document\n",
        "doc = fitz.open(\"LenderFeesWorksheetNew.pdf\")\n",
        "\n",
        "# Extract text from all pages\n",
        "text = \"\\n\".join([page.get_text() for page in doc])\n",
        "\n",
        "print(f\"Extracted {len(text.split())} words from the PDF.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwMnzwsXxzLl"
      },
      "source": [
        "Process Query (query expansion and rewriting) using OpenAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DrGUmKXjIqC"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "\n",
        "# improve query processing with query expansion\n",
        "# query expansion and rewriting implementation.\n",
        "# Initialize OpenAI LLM\n",
        "\n",
        "re_write_llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name=\"gpt-4o\",\n",
        "    max_tokens=4000,\n",
        "    api_key=userdata.get('OPENAI_API_KEY')\n",
        "    )\n",
        "\n",
        "# Create a prompt template for query rewriting\n",
        "query_rewrite_template = \"\"\"You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system.\n",
        "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
        "\n",
        "Original query: {original_query}\n",
        "\n",
        "Rewritten query:\"\"\"\n",
        "\n",
        "query_rewrite_prompt = PromptTemplate(\n",
        "    input_variables=[\"original_query\"],\n",
        "    template=query_rewrite_template\n",
        ")\n",
        "\n",
        "# Create an LLM Chain for query rewriting\n",
        "query_rewriter = query_rewrite_prompt | re_write_llm\n",
        "\n",
        "# define rewrite query function\n",
        "def rewrite_query(original_query):\n",
        "    \"\"\"\n",
        "    Rewrite the original query to improve retrieval.\n",
        "\n",
        "    Args:\n",
        "    original_query (str): The original user query\n",
        "\n",
        "    Returns:\n",
        "    str: The rewritten query\n",
        "    \"\"\"\n",
        "    response = query_rewriter.invoke(original_query)\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fyRWRYgxrdq"
      },
      "source": [
        "NEED TO IMPLEMENT SEMANTIC CHUNKING HERE!!!!\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zsvaQ2YyK_h"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.readers.file import PyMuPDFReader\n",
        "from llama_index.core.schema import Document\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Assuming 'doc' is your list of documents from PyMuPDFReader\n",
        "documents = doc\n",
        "\n",
        "# Initialize OpenAI embedding model\n",
        "embed_model = OpenAIEmbedding(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "# Create a semantic splitter for more meaningful chunks\n",
        "semantic_splitter = SemanticSplitterNodeParser(\n",
        "    buffer_size=1,\n",
        "    breakpoint_percentile_threshold=95,\n",
        "    embed_model=embed_model\n",
        ")\n",
        "\n",
        "# Create a sentence splitter for initial text splitting\n",
        "sentence_splitter = SentenceSplitter(chunk_size=512)\n",
        "\n",
        "# Convert Page objects to Document objects\n",
        "documents_converted = []\n",
        "for page in documents:\n",
        "    # Extract text from the page content\n",
        "    # Assuming PyMuPDF's Page object has a method or attribute to get text\n",
        "    # If 'get_text()' is the correct method, use it; otherwise, adjust accordingly\n",
        "    page_text = page.get_text()  # This might need to be adjusted based on PyMuPDF's API\n",
        "    doc = Document(text=page_text)\n",
        "    documents_converted.append(doc)\n",
        "\n",
        "# Apply sentence splitting to documents\n",
        "nodes = []\n",
        "for document in documents_converted:\n",
        "    # Use the sentence splitter to split the document\n",
        "    split_sentences = sentence_splitter.get_nodes_from_documents([document])\n",
        "\n",
        "    # Then, apply semantic splitting to the sentences\n",
        "    semantic_nodes = semantic_splitter.get_nodes_from_documents(split_sentences)\n",
        "    nodes.extend(semantic_nodes)\n",
        "\n",
        "# Now `nodes` contains the semantically split nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1culPQdgx513"
      },
      "source": [
        "Create hybrid retriever (best for this case)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.retrievers import BaseRetriever\n",
        "from typing import List, Optional, Tuple\n",
        "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
        "import numpy as np\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
        "\n",
        "class HybridRetriever(BaseRetriever):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vector_retriever,\n",
        "        bm25_retriever,\n",
        "        vector_weight: float = 0.5,\n",
        "        top_k_per_retriever: int = 10,\n",
        "        top_n: int = 10,\n",
        "        dedup_threshold: float = 0.9,\n",
        "    ):\n",
        "        self.vector_retriever = vector_retriever\n",
        "        self.bm25_retriever = bm25_retriever\n",
        "        self.vector_weight = vector_weight\n",
        "        self.top_k_per_retriever = top_k_per_retriever\n",
        "        self.top_n = top_n\n",
        "        self.dedup_threshold = dedup_threshold\n",
        "        super().__init__()\n",
        "\n",
        "    def _normalize_scores(self, nodes: List[NodeWithScore]) -> List[NodeWithScore]:\n",
        "        if not nodes:\n",
        "            return []\n",
        "        scores = [node.score for node in nodes]\n",
        "        min_score, max_score = min(scores), max(scores)\n",
        "        for node in nodes:\n",
        "            if max_score - min_score == 0:\n",
        "                node.score = 0.0\n",
        "            else:\n",
        "                node.score = (node.score - min_score) / (max_score - min_score)\n",
        "        return nodes\n",
        "\n",
        "    def _deduplicate_nodes(self, nodes: List[NodeWithScore]) -> List[NodeWithScore]:\n",
        "        deduped = []\n",
        "        seen_ids = set()\n",
        "        for node in nodes:\n",
        "            if node.node.node_id not in seen_ids:\n",
        "                seen_ids.add(node.node.node_id)\n",
        "                deduped.append(node)\n",
        "        return deduped\n",
        "\n",
        "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
        "        # Parallel retrieval (example using threading)\n",
        "        import threading\n",
        "        vector_results = []\n",
        "        bm25_results = []\n",
        "\n",
        "        def fetch_vector():\n",
        "            nonlocal vector_results\n",
        "            vector_results = self.vector_retriever.retrieve(query_bundle)[:self.top_k_per_retriever]\n",
        "\n",
        "        def fetch_bm25():\n",
        "            nonlocal bm25_results\n",
        "            bm25_results = self.bm25_retriever.retrieve(query_bundle)[:self.top_k_per_retriever]\n",
        "\n",
        "        t1 = threading.Thread(target=fetch_vector)\n",
        "        t2 = threading.Thread(target=fetch_bm25)\n",
        "        t1.start()\n",
        "        t2.start()\n",
        "        t1.join()\n",
        "        t2.join()\n",
        "\n",
        "        # Normalize and fuse scores\n",
        "        vector_nodes = self._normalize_scores(vector_results)\n",
        "        bm25_nodes = self._normalize_scores(bm25_results)\n",
        "\n",
        "        combined_nodes = []\n",
        "        for node in vector_nodes:\n",
        "            combined_score = self.vector_weight * node.score\n",
        "            combined_nodes.append((node, combined_score))\n",
        "        for node in bm25_nodes:\n",
        "            combined_score = (1 - self.vector_weight) * node.score\n",
        "            combined_nodes.append((node, combined_score))\n",
        "\n",
        "        # Sort by combined score and deduplicate\n",
        "        combined_nodes.sort(key=lambda x: x[1], reverse=True)\n",
        "        deduped_nodes = self._deduplicate_nodes([node for node, _ in combined_nodes])\n",
        "\n",
        "        # Apply final top_n limit\n",
        "        return deduped_nodes[:self.top_n]\n",
        "\n",
        "docstore = SimpleDocumentStore()\n",
        "docstore.add_documents(documents_converted)\n",
        "\n",
        "# Create a vector index for embedding-based retrieval\n",
        "vector_index = VectorStoreIndex.from_documents(documents_converted, embed_model=embed_model)\n",
        "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=5)\n",
        "\n",
        "# Create a BM25 keyword-based retriever\n",
        "bm25_retriever = BM25Retriever.from_defaults(docstore=docstore, similarity_top_k=3)\n",
        "\n",
        "# Combine both retrievers into a Hybrid Retriever\n",
        "hybrid_retriever = HybridRetriever(\n",
        "    vector_retriever=vector_retriever,\n",
        "    bm25_retriever=bm25_retriever,\n",
        "    # alpha=0.5\n",
        ")"
      ],
      "metadata": {
        "id": "RWkrQaFopl-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MXxWqCQyMbL"
      },
      "source": [
        "Use hybrid retrieval - need to test implementation after semantic chunking is complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJTegzwGiUvc"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set up query engine with hybrid retrieval\n",
        "llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "query_engine = RetrieverQueryEngine.from_args(hybrid_retriever, llm=llm)\n",
        "\n",
        "# Test hybrid retrieval\n",
        "query = \"What is the total estimated monthly payment?\"\n",
        "response = query_engine.query(query)\n",
        "\n",
        "print(\"Initial Response:\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUaSLLCvyV6P"
      },
      "source": [
        "then rerank vector store and get result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh0LkqAqiauO",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from llama_index.core.postprocessor import LLMRerank\n",
        "from llama_index.core.schema import QueryBundle\n",
        "\n",
        "# Initialize reranker\n",
        "reranker = LLMRerank(\n",
        "    llm=llm,\n",
        "    top_n=3,\n",
        ")\n",
        "\n",
        "# Get the retrieved results\n",
        "retrieved_nodes = response.source_nodes\n",
        "reranked_nodes = reranker.postprocess_nodes(\n",
        "    retrieved_nodes,\n",
        "    query_bundle=QueryBundle(query_str=query)\n",
        ")\n",
        "\n",
        "print(\"Top-ranked result:\", reranked_nodes[0].node.text)\n",
        "\n",
        "# Test with rewritten query\n",
        "rewritten_query = rewrite_query(query)\n",
        "print(\"Rewritten Query:\", rewritten_query)\n",
        "response_rewritten = query_engine.query(rewritten_query)\n",
        "print(\"Response with Rewritten Query:\", response_rewritten)\n",
        "\n",
        "# Rerank the results from the rewritten query\n",
        "reranked_nodes_rewritten = reranker.postprocess_nodes(\n",
        "    response_rewritten.source_nodes,\n",
        "    query_bundle=QueryBundle(query_str=rewritten_query)\n",
        ")\n",
        "print(\"Top-ranked result with Rewritten Query:\", reranked_nodes_rewritten[0].node.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query: \"How much does the borrower pay for lender's title insurance?\"\n",
        "query = \"How much does the borrower pay for lender's title insurance?\"\n",
        "response = query_engine.query(query)\n",
        "\n",
        "# Get the retrieved results\n",
        "retrieved_nodes = response.source_nodes\n",
        "reranked_nodes = reranker.postprocess_nodes(\n",
        "    retrieved_nodes,\n",
        "    query_bundle=QueryBundle(query_str=query)\n",
        ")\n",
        "\n",
        "print(\"Top-ranked result:\", reranked_nodes[0].node.text)\n",
        "\n",
        "# Test with rewritten query\n",
        "rewritten_query = rewrite_query(query)\n",
        "print(\"Rewritten Query:\", rewritten_query)\n",
        "response_rewritten = query_engine.query(rewritten_query)\n",
        "print(\"Response with Rewritten Query:\", response_rewritten)\n",
        "\n",
        "# Rerank the results from the rewritten query\n",
        "reranked_nodes_rewritten = reranker.postprocess_nodes(\n",
        "    response_rewritten.source_nodes,\n",
        "    query_bundle=QueryBundle(query_str=rewritten_query)\n",
        ")\n",
        "print(\"Top-ranked result with Rewritten Query:\", reranked_nodes_rewritten[0].node.text)\n"
      ],
      "metadata": {
        "id": "l02jHod5ttnF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}